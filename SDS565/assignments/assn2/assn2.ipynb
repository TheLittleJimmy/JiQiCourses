{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory Machine Learning: Assignment 2\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 2 is due Tuesday, October 5 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/9209/discussion/). The problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope, and as a .ipynb on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Logistic regression\n",
    "2. Regularization\n",
    "3. Stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a univariate logistic regression where we are trying to predict $Y$, which can take the value $0$ or $1$, from the variable $X$, which takes real values. Recall from lecture that we need to estimate parameters $\\beta_{0}$ and $\\beta_{1}$ by minimizing the penalized loss function:\n",
    "\n",
    "$L(\\beta_{0}, \\beta_{1}) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} \\left[ log\\left( 1 + e^{\\beta_{0} + X_{i}\\beta_{1}}\\right) - Y_{i}\\left(\\beta_{0} + X_{i}\\beta_{1}\\right)\\right] + \\lambda \\beta_{1}^{2}$ .\n",
    "\n",
    "Note that generally the intercept is not penalized.\n",
    "\n",
    "In this problem, we will implement mini-batch gradient descent. At each iteration, a random set of $m$ samples from all $n$ samples is used to calculate the loss and gradient, which is the change in the loss with respect to the parameters. We then update the estimates of $\\beta_{0}$ and $\\beta_{1}$ based on the gradient.\n",
    "\n",
    "Run the next cell to simulate data using the parameter values of $\\beta_{0} = 3$ and $\\beta_{1} = -5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000 \n",
    "np.random.seed(265)\n",
    "x1 = np.random.uniform(-5, 5, size=n) \n",
    "beta0 = 3\n",
    "beta1 = -5\n",
    "p = np.exp(beta0 + x1*beta1)/(1 + np.exp(beta0 + beta1*x1))\n",
    "y = np.random.binomial(1, p, size=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helper function for plotting we'll use later. Just run this cell; don't change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas_and_loss(beta0_all, beta1_all, loss_all, title=''):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18,5))\n",
    "    ax[0].plot(np.arange(len(beta0_all)), beta0_all)\n",
    "    ax[0].hlines(beta0, xmin=0, xmax=len(beta0_all),color = 'r')\n",
    "    ax[0].set_xlabel(\"Iteration\", fontsize=12)\n",
    "    ax[0].set_ylabel(r\"$\\widehat{\\beta}_{0}$\", fontsize=12)\n",
    "\n",
    "    ax[1].plot(np.arange(len(beta1_all)), beta1_all)\n",
    "    ax[1].hlines(beta1, xmin=0, xmax=len(beta1_all),color = 'r')\n",
    "    ax[1].set_xlabel(\"Iteration\", fontsize=12)\n",
    "    ax[1].set_ylabel(r\"$\\widehat{\\beta}_{1}$\", fontsize=12)\n",
    "    ax[1].set_title(title)\n",
    "\n",
    "    ax[2].plot(np.arange(len(loss_all)), loss_all)\n",
    "    ax[2].set_xlabel(\"Iteration\", fontsize=12)\n",
    "    ax[2].set_ylabel(\"Loss\", fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a: Deriving the updates\n",
    "\n",
    "For given values of $\\beta_{0}$ and $\\beta_{1}$ the vector $\\left( \\dfrac{\\partial}{\\partial \\beta_{0}} L_{t}(\\beta_{0}, \\beta_{1}), \\dfrac{\\partial}{\\partial \\beta_{1}} L_{t}(\\beta_{0}, \\beta_{1}) \\right)^{T}$ is called the gradient of $L_{t}(\\beta_{0}, \\beta_{1})$ and is denoted $\\nabla L_{t}(\\beta_{0}, \\beta_{1})$.\n",
    "\n",
    "Calculate the derivative of $L_{t}(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{0}$, treating $\\beta_{1}$ as a constant. (i.e. calculate $\\dfrac{\\partial}{\\partial \\beta_{0}} L_{t}(\\beta_{0}, \\beta_{1})$). \n",
    "\n",
    "Be sure to show your work by either typing it in here using LaTeX, or by taking a picture of your handwritten solutions and displaying them here in the notebook. (If you choose the latter of these two options, be sure that the display is large enough and legible. Please also upload a photo seperately to Gradescope in case the embedded image failed.)\n",
    "\n",
    "**[put your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now calculate the derivative of $L_{t}(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{1}$, treating $\\beta_{0}$ as a constant. (i.e. calculate $\\dfrac{\\partial}{\\partial \\beta_{1}} L_{t}(\\beta_{0}, \\beta_{1})$).\n",
    "\n",
    "Be sure to show your work by either typing it in here using LaTeX, or by taking a picture of your handwritten solutions and displaying them here in the notebook. (If you choose the latter of these two options, be sure that the display is large enough and legible. Please also upload a photo seperately to Gradescope in case the embedded image failed.)\n",
    "\n",
    "**[put your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we implement mini-batch stochastic gradient descent, we will use these formulas, but \n",
    "apply them to a mini-batch of $m$ randomly chosen datapoints, rather than to all $n$ datapoints \n",
    "(in our case $n=10,000$).\n",
    "Typically $m$ is chosen to be much, much smaller than $n$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b: The key ingredients\n",
    "\n",
    "Complete the function `update` in the following cell which takes values for $\\beta_{0}$ and $\\beta_{1}$, a list `inds`  containing the indexes of the $m$ selected samples, as well as a step-size $\\eta$, and returns updated values for $\\beta_{0}$ and $\\beta_{1}$ from one step of gradient descent (using all the data and your answer to Part a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(b0, b1, inds, step_size, lamb):\n",
    "\n",
    "    L_partial0 = ... # your implementation here, can be more than one line\n",
    "    L_partial1 = ... # your implementation here, can be more than one line\n",
    "    \n",
    "    b0 -= step_size * L_partial0\n",
    "    b1 -= step_size * L_partial1\n",
    "    return (b0, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the function in the next cell called `loss` which takes values for $\\beta_{0}$ and $\\beta_{1}$,\n",
    "together with a subset of indices and regularization parameter, and returns the value of the loss function evaluated at those data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(b0, b1, inds, lamb):\n",
    "    \n",
    "    output = ... # your implementation here, can be more than one line\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c: Putting it all together \n",
    "\n",
    "Now complete the implementation of `minibatch_grad_descent` which puts all of these pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_grad_descent(b0=0, b1=0, batch_size=100, step_size=10, lamb=0, iterations=1000):\n",
    "    beta0_hat = b0\n",
    "    beta1_hat = b1\n",
    "    beta0_all = []\n",
    "    beta1_all = []\n",
    "    loss_all = []\n",
    "\n",
    "    for iter in range(iterations):   \n",
    "        inds = ...  # your implementation: sample batch_size indices\n",
    "        batch_loss = ... # your implementation: call loss()\n",
    "        beta0_hat, beta1_hat = ... # your implementation: call update()\n",
    "\n",
    "        beta0_all.append(beta0_hat)\n",
    "        beta1_all.append(beta1_hat)\n",
    "        loss_all.append(batch_loss)\n",
    "        iter = iter+1\n",
    "        \n",
    "    return (beta0_hat, beta1_hat, beta0_all, beta1_all, loss_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your implementation by running the following cell, which will call the function \n",
    "with the default parameters, and then plot the beta parameters and loss during the course \n",
    "of stochastic gradient descent. We will check your plot as a first check that you have a correct implementation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, don't change it!\n",
    "\n",
    "beta0_hat, beta1_hat, beta0_all, beta1_all, loss_all = minibatch_grad_descent()\n",
    "plot_betas_and_loss(beta0_all, beta1_all, loss_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d: Assessing uncertainty\n",
    "\n",
    "We now use the code above that implements mini-batch gradient descent and \n",
    "run it several (30) times. We then display the mean and standard deviation of \n",
    "the estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell, don't change it\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "beta0_hat_all_0 = []\n",
    "beta1_hat_all_0 = []\n",
    "for rep in tqdm(range(30)):\n",
    "    beta0_hat, beta1_hat, _, _, _ = minibatch_grad_descent()\n",
    "    beta0_hat_all_0.append(beta0_hat)\n",
    "    beta1_hat_all_0.append(beta1_hat)\n",
    "    \n",
    "print('The mean of the estimated beta0 is %.2f' % np.mean(beta0_hat_all_0))\n",
    "print('The standard deviation of the estimated beta0 is %.3f' % np.std(beta0_hat_all_0))\n",
    "print('The mean of the estimated beta1 is %.2f' % np.mean(beta1_hat_all_0))\n",
    "print('The standard deviation of the estimated beta1 is %.3f' % np.std(beta1_hat_all_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on these results:\n",
    "\n",
    "1. Describe what causes the variation in the estimates.\n",
    "1. How would you construct approximate 95% confidence intervals for the estimates?\n",
    "1. Do the true parameters fall in those confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answers in this markdown cell]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e: Comparing mini-batch  sizes\n",
    "\n",
    "Repeat Part c with different $m$ (the size of the mini-batches), e.g. $m=1000$, $m=100$, and $m=10$. Use (0,0) as the initial estimates of $\\beta_{0}$ and $\\beta_{1}$. Plot $\\beta_{0}$, $\\beta_{1}$, and $L(\\beta_{0}, \\beta_{1})$ vs. iteration number. How do the plots change as you change m? Are the changes consistent with your expectation? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell, don't change it\n",
    "\n",
    "for batch_size in [10, 100, 1000]:\n",
    "    _, _, beta0_all, beta1_all, loss_all = minibatch_grad_descent(batch_size=batch_size)\n",
    "    plot_betas_and_loss(beta0_all, beta1_all, loss_all, title='mini-batch size %d' % batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on these results:\n",
    "\n",
    "1. Describe what causes the differences in the plots across the three batch sizes.\n",
    "1. Does one of the three runs give a better estimate than the others? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answers in this markdown cell]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part f: Comparing regularization levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run this cell, don't change it\n",
    "\n",
    "for lamb in [0, .001, .005]:\n",
    "    _, _, beta0_all, beta1_all, loss_all = minibatch_grad_descent(batch_size=500, lamb=lamb)\n",
    "    plot_betas_and_loss(beta0_all, beta1_all, loss_all, title = 'lambda=%.2e' % lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on these results:\n",
    "\n",
    "1. Describe what causes the differences in the plots across the three regularization levels.\n",
    "1. Does one of the three runs give a better estimate than the others? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answers in this markdown cell]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part g: Comparing step sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell, don't change it\n",
    "\n",
    "for step_size in [10, 5, 1]:\n",
    "    _, _, beta0_all, beta1_all, loss_all = minibatch_grad_descent(batch_size=100, step_size=step_size)\n",
    "    plot_betas_and_loss(beta0_all, beta1_all, loss_all, title = 'step_size=%.2f' % step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on these results:\n",
    "\n",
    "1. Describe what causes the differences in the plots across the three different step sizes.\n",
    "1. Has the run with the smallest step size converged? Why or why not?\n",
    "1. Describe how you would choose the step size and the final parameter estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Put your answers in this markdown cell]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
